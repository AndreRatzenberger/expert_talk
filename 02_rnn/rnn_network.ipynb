{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Network Demp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss: 8.9145\n",
      "Epoch [2/50], Loss: 7.8911\n",
      "Epoch [4/50], Loss: 6.6944\n",
      "Epoch [6/50], Loss: 6.5158\n",
      "Epoch [8/50], Loss: 6.2867\n",
      "Epoch [10/50], Loss: 6.1092\n",
      "Epoch [12/50], Loss: 5.9359\n",
      "Epoch [14/50], Loss: 5.7665\n",
      "Epoch [16/50], Loss: 5.6013\n",
      "Epoch [18/50], Loss: 5.4321\n",
      "Epoch [20/50], Loss: 5.2668\n",
      "Epoch [22/50], Loss: 5.1081\n",
      "Epoch [24/50], Loss: 4.9528\n",
      "Epoch [26/50], Loss: 4.8053\n",
      "Epoch [28/50], Loss: 4.6668\n",
      "Epoch [30/50], Loss: 4.5343\n",
      "Epoch [32/50], Loss: 4.4090\n",
      "Epoch [34/50], Loss: 4.2908\n",
      "Epoch [36/50], Loss: 4.1783\n",
      "Epoch [38/50], Loss: 4.0708\n",
      "Epoch [40/50], Loss: 3.9682\n",
      "Epoch [42/50], Loss: 3.8692\n",
      "Epoch [44/50], Loss: 3.7734\n",
      "Epoch [46/50], Loss: 3.6803\n",
      "Epoch [48/50], Loss: 3.5898\n",
      "Generated text:\n",
      "kinder und  der der der der der der der der wirth kam ein und als es sich auf den weg und sprach der junge könig und sprach er zu dem backofen gelangte und sprach der junge könig und sprach er zu dem backofen gelangte und sprach der junge könig und sprach er\n"
     ]
    }
   ],
   "source": [
    "# Simple Text Generating RNN using PyTorch (Word-Level)\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Read text data from a file\n",
    "file_path = 'data/grimm.txt'  # Replace with your text file path\n",
    "text = read_text(file_path)\n",
    "\n",
    "# Preprocess text: Convert to lowercase and split into words\n",
    "text = text.lower()\n",
    "words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Add a special token for unknown words\n",
    "words_set = sorted(set(words))\n",
    "words_set.append('<unk>')  # Add unknown token to the set\n",
    "word_to_index = {word: i for i, word in enumerate(words_set)}\n",
    "index_to_word = {i: word for i, word in enumerate(words_set)}\n",
    "\n",
    "# Prepare the dataset\n",
    "maxlen = 10  # Length of extracted word sequences\n",
    "step = 1     # Step size for creating sequences\n",
    "sentences = []\n",
    "next_words = []\n",
    "\n",
    "for i in range(0, len(words) - maxlen, step):\n",
    "    sentences.append(words[i: i + maxlen])\n",
    "    next_words.append(words[i + maxlen])\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = np.zeros((len(sentences), maxlen), dtype=np.int64)  # Changed to int64 for indices\n",
    "y = np.zeros((len(sentences), len(words_set)), dtype=np.float32)  # Remain float for the target\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t] = word_to_index[word]\n",
    "    y[i, word_to_index[next_words[i]]] = 1\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)  # Ensure the input tensor is of type Long\n",
    "y = torch.tensor(y, dtype=torch.float32)  # Ensure the target tensor is of type Float\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Simple RNN layer\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Pass input through embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # Pass embedding through RNN\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # Pass RNN output through fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)\n",
    "\n",
    "# Set parameters\n",
    "input_size = len(words_set)\n",
    "hidden_size = 128\n",
    "output_size = len(words_set)\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(X.size(0))\n",
    "    outputs, hidden = model(X, hidden)\n",
    "    loss = criterion(outputs, torch.argmax(y, dim=1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(model, seed_text, length):\n",
    "    model.eval()\n",
    "    generated = seed_text\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    seed_words = seed_text.lower().split()\n",
    "    for _ in range(length):\n",
    "        x_pred = torch.zeros(1, maxlen, dtype=torch.long)\n",
    "        for t, word in enumerate(seed_words[-maxlen:]):\n",
    "            if word in word_to_index:\n",
    "                x_pred[0, t] = word_to_index[word]\n",
    "            else:\n",
    "                x_pred[0, t] = word_to_index['<unk>']  # Use <unk> if the word is not in the index\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(x_pred, hidden)\n",
    "            next_index = torch.argmax(output).item()\n",
    "            next_word = index_to_word[next_index]\n",
    "            \n",
    "        generated += ' ' + next_word\n",
    "        seed_words.append(next_word)\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Generate text using the trained model\n",
    "seed_text = \"kinder und \"\n",
    "generated_text = generate_text(model, seed_text, 50)\n",
    "print(f\"Generated text:\\n{generated_text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
